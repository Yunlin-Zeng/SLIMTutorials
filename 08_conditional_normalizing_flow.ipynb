{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Conditional Normalizing Flow for Joint and Conditional sampling. \n",
    "This notebook is the second in the tutorial series for InvertibleNetworks.jl. In this tutorial, we will show how to train a Normalizing Flow (NF) to sample from a joint distribution $p(x,y)$. Pairing joint distribution training with conditional normalizing flows allows us to sample from the conditional distribution and therefore the posterior distribution of an inverse problem. We will use the conditional normalizing flow architecture given by [HINT (Kruse, Jakob, et al.)](https://arxiv.org/pdf/1905.10687.pdf).\n",
    "\n",
    "The HINT architecture implements:\n",
    "* HINT recursive couplying layer for increased expressiveness\n",
    "* HINT conditional couplying layer for conditional samplying\n",
    "* Other layers: ActNorms, 1x1 Convolutions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.develop(\"InvertibleNetworks\") #note only need to run once to install\n",
    "import Pkg; Pkg.add(\"Flux\")                   #note only need to run once to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using InvertibleNetworks\n",
    "using LinearAlgebra\n",
    "using PyPlot\n",
    "using Flux\n",
    "using Random\n",
    "\n",
    "PyPlot.rc(\"font\", family=\"serif\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior distribution - Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 1\n",
    "ny = 1\n",
    "n_in = 2\n",
    "n_train = 60000;\n",
    "X_train = sample_banana(n_train);\n",
    "size(X_train) #(nx, ny, n_channels, n_samples) Note: we put 2 dimensions as channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 500\n",
    "\n",
    "fig = figure(figsize=(10,5)); \n",
    "subplot(1,2,1); title(L\"x \\sim p_x(x)\")\n",
    "scatter(X_train[1,1,1,1:plot_num], X_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"x \\sim p_{x}(x)\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "xlim(-4,4); ylim(0,30);\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear inverse problem with Gaussian additive noise\n",
    "\n",
    "As a stylized example, we will solve an inverse problem given linear measurements of a model $x$ through a compressive sensing operator $A$. We will also add gaussian additive noise and the result will be our observed data.   \n",
    "$$y = Ax + \\epsilon$$\n",
    "\n",
    "Even though we will chose an $A$ that will be invertible, the added noise makes this problem ill-posed. Therefore for a given datapoint $y$ there is not a single model $x$ which solves the inverse problem. Instead there is a whole distribution of models $p(x|y)$, the conditional distribution. In this context, this is also called the posterior distribution. \n",
    "\n",
    "Note: we could also use a non-linear operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make an our linear measurement operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "Random.seed!(8)\n",
    "\n",
    "# Linear forward operator\n",
    "A = randn(Float32,2,2)\n",
    "A = A / (2*opnorm(A)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use this operator to make training data $Y_{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = reshape(A*reshape(X_train, :, n_train), nx, ny, n_in, n_train)\n",
    "Y_train += .2f0*randn(Float32, nx, ny, n_in, n_train)\n",
    "\n",
    "fig = figure(figsize=(10,5)); \n",
    "subplot(1,2,1); title(L\"x \\sim p_{x,y}(x,y)\")\n",
    "scatter(X_train[1,1,1,1:plot_num], X_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"x \\sim p_{x,y}(x,y)\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "xlim(-4,4); ylim(0,30);\n",
    "legend();\n",
    "\n",
    "subplot(1,2,2); title(L\"Noisy data $y = Ax + \\epsilon$\")\n",
    "scatter(Y_train[1,1,1,1:plot_num], Y_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"y \\sim p_{x,y}(x,y)\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a conditional normalizing flow. \n",
    "\n",
    "A conditional normalizing flow is trained by learning the joint distribution $p(x,y)$ this is done in the same way as a non-conditional normalizing flow is trained my minimizing the negative log likelihood (Refer to Tutorial notebook 01 for more details. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(H, X, Y)\n",
    "    batch_size = size(X)[end] \n",
    "    \n",
    "    Zx, Zy, lgdet = H.forward(X, Y)\n",
    "    l2_loss = 0.5*norm(tensor_cat(Zx, Zy))^2 / batch_size  #likelihood under Normal Gaussian training \n",
    "    \n",
    "    #gradients under Normal Gaussian training\n",
    "    dZx = Zx / batch_size \n",
    "    dZy = Zy / batch_size \n",
    "    \n",
    "    H.backward(dZx, dZy, Zx, Zy) #sets gradients of G wrt output and also logdet terms\n",
    "    \n",
    "    return (l2_loss, lgdet)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "\n",
    "n_hidden = 64\n",
    "batchsize = 64\n",
    "depth = 10\n",
    "\n",
    "# Construct HINT network\n",
    "H = NetworkConditionalHINT(n_in, n_hidden, depth; k1=1, k2=1, p1=0, p2=0)\n",
    "\n",
    "# Training\n",
    "maxiter = 900\n",
    "lr      = 5f-4\n",
    "lr_decay_step = 90\n",
    "\n",
    "#compose adam optimizer with exponential learning rate decay \n",
    "opt = Flux.Optimiser(ExpDecay(lr, .9f0, lr_decay_step, 1f-6), Flux.ADAM(lr))\n",
    "\n",
    "loss_l2_list    = zeros(maxiter)\n",
    "loss_lgdet_list = zeros(maxiter)\n",
    "\n",
    "for j=1:maxiter\n",
    "\n",
    "    # Evaluate objective and gradients\n",
    "    X = sample_banana(batchsize)\n",
    "    Y = reshape(A*reshape(X, :, batchsize), nx, ny, n_in, batchsize)\n",
    "    Y += .2f0*randn(Float32, nx, ny, n_in, batchsize)\n",
    "\n",
    "    losses = loss(H, X, Y)\n",
    "    loss_l2_list[j]    = losses[1]\n",
    "    loss_lgdet_list[j] = losses[2]\n",
    "    \n",
    "    print(\"Iter : iteration=\", j, \"/\", maxiter, \", batch=\", \n",
    "            \"; f l2 = \",   loss_l2_list[j], \n",
    "            \"; f lgdet = \",loss_lgdet_list[j], \n",
    "            \"; f nll objective = \",loss_l2_list[j] - loss_lgdet_list[j], \"\\n\")\n",
    "\n",
    "    # Update params\n",
    "    for p in get_params(H)\n",
    "        Flux.update!(opt, p.data, p.grad)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check training objective log \n",
    "There are various ways to train a NF:\n",
    "- train your network to convergence of objective\n",
    "- use earlystopping to prevent overfitting \n",
    "- check normality of $Zx, Zy = H_{\\theta}(Z,Y)$ with qq plots \n",
    "- as a heuristic simply observe $Zx, Zy = H_{\\theta}(X, Y)$ until it looks normal under the eyeball norm. \n",
    "\n",
    "Note: joint distributions are harder to learn than marginal distributions. There are a few reasons including the larger dimensionality and the fact that learning a joint distribution implicitly also learns a conditional distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gt_l2 = 0.5*nx*ny*n_in*2 #l2 norm of noise. Note: extra 2 factor since learning a 2 rv. joint distribution\n",
    "\n",
    "fig, axs = subplots(3, 1, sharex=true, figsize=(10,7))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "axs[1].plot(loss_l2_list, color=\"black\", linewidth=0.6); \n",
    "axs[1].axhline(y=gt_l2,color=\"red\",linestyle=\"--\",label=\"Normal Noise Likelihood\")\n",
    "axs[1].set_ylabel(\"L2 Norm\")\n",
    "axs[1].yaxis.set_label_coords(-0.05, 0.5)\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(loss_lgdet_list, color=\"black\", linewidth=0.6); \n",
    "axs[2].set_ylabel(\"Log DET\") \n",
    "axs[2].yaxis.set_label_coords(-0.05, 0.5) \n",
    "\n",
    "axs[3].plot(loss_l2_list - loss_lgdet_list, color=\"black\", linewidth=0.6); \n",
    "axs[3].set_ylabel(\"Full Objective\") \n",
    "axs[3].yaxis.set_label_coords(-0.05, 0.5)\n",
    "axs[3].set_xlabel(\"Parameter Update\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a Conditional Normalizing Flow \n",
    "\n",
    "The first step in testing a normalizing flow is to understand what prior it has learned. In this case, we trained our network to generate samples from a joint distribution so we will first check that these generative samples look like \n",
    " \n",
    "We start with normal gaussian variables $Zx, Zy \\sim N(0, I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 500;\n",
    "Zx_test = randn(Float32,nx,ny,n_in, num_test_samples);\n",
    "Zy_test = randn(Float32,nx,ny,n_in, num_test_samples);\n",
    "\n",
    "fig = figure(figsize=(10,5)); \n",
    "ax1 = subplot(1,2,1)\n",
    "scatter(Zx_test[1,1,1,:], Zx_test[1,1,2,:]; alpha=0.4, color=\"black\", label = L\"Zx \\sim p_{z}(z)\");\n",
    "xlabel(L\"Zx_1\"); ylabel(L\"Zx_2\"); xlim(-5,5); ylim(-5,5);\n",
    "legend(); ax1.set_aspect(1);\n",
    "\n",
    "ax2 = subplot(1,2,2)\n",
    "scatter(Zy_test[1,1,1,:], Zy_test[1,1,2,:]; alpha=0.4, color=\"black\", label = L\"Zy \\sim p_{z}(z)\");\n",
    "xlabel(L\"Zy_1\"); ylabel(L\"Zy_2\"); xlim(-5,5); ylim(-5,5);\n",
    "legend(); ax2.set_aspect(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make generative samples from the joint distribution by passing both Z variables through the inverse network $ x,  y = H^{-1}_\\theta(Zx,Zy)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = H.inverse(Zx_test, Zy_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_num = 150\n",
    "start_points = [(Zx_test[1,1,1,i], Zx_test[1,1,2,i]) for i in 1:trans_num]\n",
    "end_points = [(X_test[1,1,1,i], X_test[1,1,2,i]) for i in 1:trans_num]\n",
    "\n",
    "# Plot transformed Latent Zx\n",
    "fig = figure(figsize=(10,10));\n",
    "ax = fig.add_subplot(1,2,1); title(L\"Transformed latent $Zx \\rightarrow X=H^{-1}_\\theta(Zx, Zy)$\");\n",
    "\n",
    "for line in zip(start_points, end_points)\n",
    "    plot([line[1][1],line[2][1]], [line[1][2] ,line[2][2]], alpha=0.2, linewidth=0.3, color=\"black\")\n",
    "    scatter(line[1][1], line[1][2], marker=\"o\",alpha=0.4, color=\"black\")\n",
    "    scatter(line[2][1], line[2][2], marker=\"o\",alpha=0.4, color=\"orange\")\n",
    "end\n",
    "xlabel(\"First Dimension X\"); ylabel(\"Second Dimension X\");\n",
    "ylim(-2.5,20); xlim(-2.5,2.5); ax.set_aspect(1)\n",
    "\n",
    "# Plot transformed Latent Zy\n",
    "start_points = [(Zy_test[1,1,1,i], Zy_test[1,1,2,i]) for i in 1:trans_num]\n",
    "end_points = [(Y_test[1,1,1,i], Y_test[1,1,2,i]) for i in 1:trans_num]\n",
    "\n",
    "ax = fig.add_subplot(1,2,2); title(L\"Transformed latent $Zy \\rightarrow Y=H^{-1}_\\theta(Zy)$\");\n",
    "for line in zip(start_points, end_points)\n",
    "    plot([line[1][1],line[2][1]], [line[1][2] ,line[2][2]], alpha=0.2, linewidth=0.3, color=\"black\")\n",
    "    scatter(line[1][1], line[1][2], marker=\"o\",alpha=0.4, color=\"black\")\n",
    "    scatter(line[2][1], line[2][2], marker=\"o\",alpha=0.4, color=\"orange\")\n",
    "end\n",
    "xlabel(\"First Dimension Y\"); ylabel(\"Second Dimension Y\");\n",
    "ax.set_aspect(1)\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually compare generative joint samples with joint samples from the ground truth density $x, y \\sim p(x,y) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(10,5)); \n",
    "subplot(1,2,1); title(L\"x \\sim p(x)\")\n",
    "scatter(X_train[1,1,1,1:plot_num], X_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"x \\sim p(x)\");\n",
    "scatter(X_test[1,1,1,1:plot_num], X_test[1,1,2,1:plot_num]; alpha=0.4, color=\"orange\", label = L\"x \\sim p_{\\theta}(x) = H_\\theta^{-1}(Zx, Zy)\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "xlim(-4,4); ylim(0,30);\n",
    "legend();\n",
    "\n",
    "subplot(1,2,2); title(L\"Noisy data $y = Ax + \\epsilon$\")\n",
    "scatter(Y_train[1,1,1,1:plot_num], Y_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"y \\sim p(y)\");\n",
    "scatter(Y_test[1,1,1,1:plot_num], Y_test[1,1,2,1:plot_num]; alpha=0.4, color=\"orange\", label = L\"y \\sim p_{\\theta}(y) = H_\\theta^{-1}(Zy)\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference of inverse problem given noisy data\n",
    "\n",
    "After verifying that training was successful we can proceed to perform inference of the inverse problem. We will first observe a single data $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star  = sample_banana(1);\n",
    "y_obs = reshape(A*vec(x_star), nx, ny, n_in, 1);\n",
    "y_obs += .2f0*randn(Float32, size(y_obs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(10,5)); \n",
    "subplot(1,2,1); title(L\"Ground truth model $x^{*}$\")\n",
    "scatter(X_train[1,1,1,1:plot_num], X_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"x \\sim p(x,y)\");\n",
    "scatter(x_star[1,1,1,1], x_star[1,1,2,1]; marker=\"*\", color=\"black\", label = L\"x^{*}\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "xlim(-4,4); ylim(0,30);\n",
    "legend();\n",
    "\n",
    "subplot(1,2,2); title(L\"Observed Noisy data $y_{obs} = Ax^{*} + \\epsilon$\")\n",
    "scatter(Y_train[1,1,1,1:plot_num], Y_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"y \\sim p(x,y)\");\n",
    "scatter(y_obs[1,1,1,1], y_obs[1,1,2,1]; marker=\"*\", color=\"red\", label = L\"y_{obs}\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form conditional distribution given observed data $p(x|y_{obs})$\n",
    "\n",
    "Remember that the y-lane in HINT is completely independent of the x-lane. This functionality is implemented in  ``H.forward_Y`` which we will call on our observed data to get the corresponding latent variable $z_y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zy_fixed = H.forward_Y(y_obs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can sample directly from the conditional aka posterior distribution $x \\sim p(x|y_{obs})$ We do this by resampling $z_x \\sim N(0,I)$ and calling the inverse network $H^{-1}(z_x, z_y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix zy and resample zx many times to generate samples from the conditional distribution p(x|y)\n",
    "cond_sampling_size = 50\n",
    "Zx = randn(Float32, nx, ny, n_in, cond_sampling_size)\n",
    "X_post = H.inverse(Zx, zy_fixed.*ones(Float32, nx, ny, n_in, cond_sampling_size))[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(15,5)); \n",
    "subplot(1,3,1); title(L\"Ground truth model $x^{*}$\")\n",
    "scatter(X_train[1,1,1,1:plot_num], X_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"x \\sim p(x,y)\");\n",
    "scatter(x_star[1,1,1,1], x_star[1,1,2,1]; marker=\"*\", color=\"black\", label = L\"x^{*}\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "xlim(-4,4); ylim(0,30);\n",
    "legend();\n",
    "\n",
    "subplot(1,3,2); title(L\"Observed Noisy data $y_{obs} = Ax^{*} + \\epsilon$\")\n",
    "scatter(Y_train[1,1,1,1:plot_num], Y_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"y \\sim p(x,y)\");\n",
    "scatter(y_obs[1,1,1,1], y_obs[1,1,2,1]; marker=\"*\", color=\"red\", label = L\"y_{obs}\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "legend();\n",
    "\n",
    "subplot(1,3,3); title(L\"Conditional Distribution given observed data $y_{obs}$\")\n",
    "scatter(X_train[1,1,1,1:plot_num], X_train[1,1,2,1:plot_num]; alpha=0.4, label = L\"x \\sim p(x,y)\");\n",
    "scatter(X_post[1,1,1,:], X_post[1,1,2,:]; alpha=0.4, color=\"red\", label = L\"x \\sim p_{\\theta}(x | y_{obs})\");\n",
    "scatter(x_star[1,1,1,1], x_star[1,1,2,1]; marker=\"*\",color=\"black\",  label = L\"x^{*}\");\n",
    "xlabel(L\"x_1\"); ylabel(L\"x_2\");\n",
    "xlim(-4,4); ylim(0,30);\n",
    "legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
